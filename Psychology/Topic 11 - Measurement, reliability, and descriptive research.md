### **1. Measurement Scales**

Measurement is a systematic way of assigning numbers or names to objects and their features. The choice of scale determines both the kind of statistical analyses that can be conducted and the conclusions that can be drawn.

#### **Types of Scales:**

#trytorecall 

- **Nominal Scale**: Differences between categories without any numeric meaning (e.g., gender, nationality). Statistical operations like counting frequencies are possible.
- **Ordinal Scale**: Data is ranked in order, but intervals between ranks are not equal (e.g., race positions). Median and mode can be calculated.
- **Interval Scale**: Equal intervals between values, but no true zero (e.g., temperature in Celsius). Operations like addition and subtraction are possible.
- **Ratio Scale**: Similar to interval scales but with an absolute zero (e.g., height, weight). Allows for a full range of mathematical operations, including ratios.

##### **Why is the Choice of Scale Important?**

The chosen measurement scale dictates the types of statistical tests you can apply and the conclusions you can draw. 
	For example, you cannot calculate an average for nominal data, and ratio scales allow for more complex analysis like calculating proportions or making statements about relative quantities.

---

### %% fold %% **2. Reliability**

**Reliability** refers to the consistency of a measure. A reliable test will give consistent results under consistent conditions. Reliability can be affected by both **random** and **systematic errors**.

- **Observed score = True score + Error score**
    - **True score**: The real value being measured.
    - **Error score**: The difference between the true score and the observed score. It can be due to measurement errors, test conditions, or the participant's temporary state.

#### **Types of Reliability:**

#trytorecall 

1. **Test-retest Reliability**: The consistency of results when the same test is administered at two different points in time.
2. **Internal Reliability**: Consistency of results across items within the test.
    - **Split-half reliability**: Dividing the test into two halves and comparing the results.
    - **Cronbach’s Alpha**: Measures internal consistency by calculating the correlations between all items in the test.
3. **Interrater Reliability**: Agreement between different observers or raters.
4. **Experimental Reliability**: Consistency of results across repeated experiments or trials.

---

### %% fold %% **3. Descriptive Research**

Descriptive research aims to describe behaviors or phenomena through observation, surveys, or case studies. It is crucial in making **frequency claims**, such as “three in four people are dissatisfied with their jobs.”

#### **Types of Descriptive Research:**

#trytorecall 

1. **Observational Research**: Systematically recording behaviors or events as they naturally occur.
    
    - **Problems**:
        - **Interrater reliability** may be low if observers don't agree.
        - **Observer bias**: The observer may unintentionally influence the results to fit expectations.
        - **Reactivity**: Subjects may alter their behavior if they know they are being watched.
    - **Solutions**: Use multiple observers, employ unobtrusive measures, and adopt strategies like “waiting it out” so that subjects return to natural behavior.
2. **Survey Research**: A method of posing questions to participants (via interviews, questionnaires, or online) to collect data about attitudes, behaviors, or experiences.
    
    - **Problems**:
        - Low internal validity due to potential biases in question wording (e.g., leading or double-barreled questions), order of questions, and response sets (e.g., yea-saying or fence-sitting).
        - Participants may also **fake good** to appear socially desirable.
    - **Solutions**: Ensure questions are well-worded and neutrally framed. Using anonymous surveys can reduce social desirability bias.
3. **Case Studies**: In-depth analyses of a single individual or small group, which can generate rich qualitative data.
    
    - **Limitations**: Cannot generalize findings to the broader population.

#### **Frequency Claims**

- Descriptive research often leads to **frequency claims**, which state how common a particular behavior or phenomenon is within a population.
	- For frequency claims to be valid, the sample must be representative of the population, not biased.


### %% fold %% **4. Errors in Measurement**

When measuring a construct, there are two primary types of errors: **random errors** and **systematic errors**. These errors impact the reliability and validity of the measurements.

#### **Types of Errors:**

#trytorecall 

1. **Random Errors**: These are unpredictable fluctuations in measurement that affect the observed score. Random errors might arise from temporary factors such as mood, fatigue, or environmental disturbances during the testing.
    - Example: A participant may be more tired during one test session than another, leading to inconsistent responses.
    - **Impact**: Reduces the reliability of the measurement but does not necessarily bias the results in one direction.
2. **Systematic Errors**: These occur consistently in the same direction and affect the validity of the measurement. Systematic errors could result from poorly designed tests, biased questions, or issues with the measurement instrument.
    - Example: A broken thermometer that always reads 2 degrees higher than the actual temperature.
    - **Impact**: Reduces the validity of the results as it introduces a consistent bias.

---

### %% fold %% **5. More on Descriptive Research**

Descriptive research methods provide foundational data about behavior but face several challenges, particularly in relation to the validity of the claims made. There are three key forms of descriptive research:

#### **1. Observational Research** (continued)

- Observational research is crucial for studying behaviors that are difficult to capture through self-report methods. However, the methodology needs to account for the following:
    
    - **Observer Bias**: When the expectations or beliefs of the observer influence the way they record behaviors.
        
        - **Solution**: Use clear coding schemes, train observers well, and employ multiple raters to ensure consistency.
    - **Observer Effects (Expectancy Effects)**: When the observer's presence or behavior inadvertently influences the subject’s behavior.
        
        - **Solution**: Use **unobtrusive observations** (e.g., hidden cameras) or the **wait-it-out strategy**, where observers wait until the participants become accustomed to their presence.
    - **Reactivity**: This occurs when participants alter their behavior simply because they know they are being observed.
        
        - **Solution**: Use **unobtrusive measures**, such as indirect observation of traces left by behavior (e.g., worn-out paths in a park to indicate foot traffic).

#### **2. Survey Research** (continued)

- Surveys are widely used to gather data about attitudes, beliefs, and behaviors. However, researchers must take care to minimize biases introduced by poorly designed questions.
    
    - **Question Wording**:
        
        - **Leading Questions**: A question that suggests a particular answer. Example: "How fast was the car going when it smashed into the other car?" compared to "How fast was the car going when it hit the other car?"
        - **Double-Barreled Questions**: Asking two questions in one. Example: "Do you enjoy swimming and wearing sunscreen?" The respondent may enjoy one but not the other.
        - **Negatively Worded Questions**: These can confuse respondents and lead to inaccurate answers. Example: "People who do not drive with a suspended license should never be punished."
    - **Question Order**: The sequence of questions can influence responses. If an earlier question primes a respondent to think in a certain way, it may affect their answers to subsequent questions.
        
    - **Response Biases**:
        
        - **Acquiescence (Yea-Saying)**: Participants may agree with statements regardless of their actual feelings. This can be reduced by including reverse-worded questions.
        - **Fence Sitting**: Respondents may choose the middle option in Likert scales to avoid taking a stance on controversial issues. Removing the neutral option can help mitigate this issue but may frustrate those who genuinely have no opinion.

#### **3. Case Studies** (continued)

- Case studies provide in-depth, detailed analysis of an individual or small group. While they offer rich qualitative data, they are limited in their ability to generalize findings to a larger population.
    
    - **Use**: Case studies are useful for exploring new areas of research and generating hypotheses, but they are less effective for testing these hypotheses on a broader scale.

---

### %% fold %% **6. Evaluating Construct Validity in Descriptive Research**

**Construct Validity** refers to how well a test or measurement captures the concept it is intended to measure. For frequency claims, which are common in descriptive research, construct validity is critical. Researchers evaluate this by assessing the quality of their measures and the appropriateness of their operational definitions.

#### **Types of Measures**:

#trytorecall 

1. **Self-Report Measures**: Participants report their own behaviors, attitudes, or feelings, usually through questionnaires or interviews.
    - **Example**: Asking participants to rate their happiness on a scale from 1 to 7.
    - **Validity Concern**: Self-reports may suffer from **social desirability bias** or **memory recall issues**, where participants may underreport undesirable behaviors or inaccurately recall events.
2. **Observational Measures**: Observing and recording behavior, either directly or through video/audio recordings.
    - **Example**: Counting how many times a participant smiles during a social interaction to measure happiness.
    - **Validity Concern**: Observer bias or reactivity may affect the accuracy of the observations.
3. **Physiological Measures**: Recording biological data such as heart rate, hormone levels, or brain activity.
    - **Example**: Measuring cortisol levels to assess stress.
    - **Validity Concern**: Physiological measures can be objective but may not always directly correspond to the psychological construct being studied (e.g., cortisol levels might be influenced by factors other than stress).

#### **Ensuring Construct Validity**:

- Using multiple measures (self-report, observational, physiological) to assess the same construct can provide more robust evidence for construct validity.
- **Convergent validity**: When different measures of the same construct (e.g., self-report and physiological) provide consistent results.
- **Discriminant validity**: Demonstrating that the measure is not capturing unrelated constructs.

---

### %% fold %% **7. Frequency Claims and External Validity**

**Frequency Claims**: These are claims about how common a particular behavior or trait is in a population. Examples include statements like "60% of people are satisfied with their jobs" or "3 out of 4 people experience stress."

- **External Validity**: Refers to the extent to which the results of a study can be generalized to the broader population. For frequency claims to be credible, the sample must be representative of the population of interest.
    
    - **Biased Samples**: A biased sample occurs when some members of the population have a higher probability of being included in the study than others, threatening external validity.
        - Example: Surveying only college students to make conclusions about the general adult population.
    - **Representative Samples**: A representative sample is one where every member of the population has an equal chance of being included.
        - **Techniques**: Researchers use random sampling, stratified sampling, and replication to ensure the external validity of their findings.



--- 

## My notes 

Measurement scales 
- Nominal scale - differences (categorical)
- Ordinal scale - ranked order 
- Interval scale - equal intervals (add/subst)
- Ratio scale - absolute zero (ratio's)

- Why imp?

Reliability
- Observed score (measured) = true score (real) + error score (difference b/w the measured and true)
- Observed score = True ability + random error

- Errors 
	- Random 
	- Systematic

Reliability 

(Retrospective-self-report tests have low reliability )

- Test reliability:
	- Test-retest
	- Internal 
		- Split-half
		- Cronbach's alpha (compute all of the correlations b/w all of the items)
- Interrater 
- Experimental 

Descriptive research - describe behaviour 
- Observational
	- Problems? - Solutions
- Survey (same problems as self-reports)
	- Formats?
	- Problems? - Solutions 
- Case studies (can't make generalisations)

- Freq claims?
