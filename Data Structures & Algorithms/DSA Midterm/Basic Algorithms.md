# Search Algorithms

### Linear Search

- Scan elements one by one until found.
    
- **Best case**: first element â†’ `O(1)`.
    
- **Worst case**: not found â†’ `O(n)`.
    
#### Loop Invariant
At step `i`, element `x` has not been found in `A[1..i-1]`.

---
## Sorting Algorithms

### Insertion Sort

- Works like sorting a **hand of cards**.
    
- Key idea: Maintain a **sorted subarray** `A[1..i-1]`, insert `A[i]` in correct place.

##### Pseudocode:
``` 
def insertion_sort(A):
    for i in range(1, len(A)):
        key = A[i]
        j = i - 1
        while j >= 0 and A[j] > key:
            A[j+1] = A[j]
            j -= 1
        A[j+1] = key
```
- **Best case**: already sorted â†’ `O(n)`.
    
- **Worst case**: reverse order â†’ `O(n^2)`.
    
- **Exam Tip**: Always mention **loop invariants** (init, maintenance, termination).

---

### Merge Sort (Divide & Conquer)

- Split â†’ Sort subarrays â†’ Merge back.
    
- Recurrence: `T(n) = 2T(n/2) + Î˜(n)`.
    
- Solution: `Î˜(n log n)`.
    

##### Pseudocode:
```
def merge_sort(A):
    if len(A) <= 1:
        return A
    mid = len(A)//2
    L = merge_sort(A[:mid])
    R = merge_sort(A[mid:])
    return merge(L, R)

```

**Analogy:** Sorting **papers** by splitting piles, sorting each, then merging.

## Divide & Conquer (Book - Ch. 3)

### Paradigm

1. Divide â†’ Split into subproblems.
    
2. Conquer â†’ Solve recursively.
    
3. Combine â†’ Merge solutions.
    

### Classic Examples

- **MergeSort** :
    
    - Recurrence: `T(n) = 2T(n/2) + Î˜(n)` â†’ Î˜(n log n).
        
- **Counting Inversions**:
    
    - Brute force = Î˜(nÂ²).
        
    - D&C piggybacks on MergeSort â†’ Î˜(n log n).
        
- **Strassenâ€™s Matrix Multiplication**:
    
    - Improves over naÃ¯ve O(nÂ³) â†’ ~O(n^2.81). (Nice-to-know, not core).
        

âš ï¸ Exam tip: If asked â€œgive recurrenceâ€ â†’ just write it, donâ€™t solve unless asked.

## The Master Method (Book - Ch. 4)

A shortcut for solving divide & conquer recurrences:

For recurrences like:  
`T(n) = aT(n/b) + f(n)`

- Compare `f(n)` with `n^(log_b a)`.
    
- Three cases:
    
    1. If `f(n)` is smaller â†’ T(n) = Î˜(n^(log_b a)).
        
    2. If equal â†’ T(n) = Î˜(n^(log_b a) log n).
        
    3. If bigger â†’ T(n) = Î˜(f(n)).
        

ğŸ‘‰ Example: MergeSort â†’ `T(n) = 2T(n/2) + Î˜(n)` â†’ case 2 â†’ Î˜(n log n).

âš ï¸ Exam tip: Write **which case applies**.

---
### Quicksort

- Choose pivot, partition into `< pivot`, `>= pivot`, then recurse.
    
- **Best case** (balanced partitions): `Î˜(n log n)`.
    
- **Worst case** (already sorted, bad pivot choice): `Î˜(n^2)`.
    

**Exam Tip:**

- Worst case occurs when pivot = min or max.
    
- Best case occurs when pivot splits evenly.

#### More On Quicksort (Book -Ch. 5)

- **Partition step**: Rearrange elements around pivot.
    
- **Best case**: Balanced partitions â†’ Î˜(n log n).
    
- **Worst case**: Pivot = smallest/largest every time â†’ Î˜(nÂ²).
    
- **Randomized Quicksort**:
    
    - Picks pivot randomly.
        
    - Expected time complexity: Î˜(n log n).
        

âš ï¸ Exam tip: If asked â€œwhen does worst case occur?â€ â†’ say: _when pivot is min/max â†’ unbalanced partition_

---
### Heapsort

- Build max heap â†’ repeatedly extract max.
    
- Always `Î˜(n log n)`.
    
- Uses heap property: parent â‰¥ children.

---
### Counting Sort

- Non-comparison sort. Works when keys âˆˆ `[0..k]`.
    
- Complexity: `Î˜(n + k)`.
    
Stable sort â€“ order of equal keys preserved (important for radix sort correctness).


---
## Non-Comparison Sorts (Book - Ch. 5.6)

These **beat n log n** under assumptions :

- **Counting Sort** â€“ works if keys âˆˆ [0..k], runs in Î˜(n + k), stable.
    
- **Bucket Sort** â€“ uniform distribution [0,1), avg Î˜(n).
    
- **Radix Sort** â€“ sorts digits using Counting Sort, good for large integers.
    

âš ï¸ Exam tip: Mention **stability** when relevant â†’ Radix Sort fails if subroutine is not stable


## Randomized Algorithms (Ch. 5)

- Randomness avoids bad worst cases.
    
- Quicksort example: pivot chosen randomly â†’ avg = Î˜(n log n).
    
- Uses **probability analysis** (expectation, linearity).
    

âš ï¸ Exam tip: You donâ€™t need deep probability, just know why random pivot = balanced splits on average.


## Selection Algorithms (Ch. 6)

Goal: find the k-th smallest element.

- **Randomized Selection (RSelect)**:
    
    - Partition like Quicksort.
        
    - Expected Î˜(n).
        
- **Deterministic Selection (DSelect, Median of Medians)**:
    
    - Guarantees Î˜(n) worst case.
---
## Time Complexity
*!Memorise for the exam*!

**Quizlet Flashcards for memorisation:**
[Big-O Algorithms Time Complexity](https://quizlet.com/81400870/big-o-algorithms-time-complexity-flash-cards/)

## Asymptotic Notation (Ch. 2)

- **Big-O (`O`)**: Upper bound â€“ algorithm never worse than this (like â‰¤).
    
- **Big-Î© (`Î©`)**: Lower bound â€“ algorithm never better than this (like â‰¥).
    
- **Big-Î˜ (`Î˜`)**: Tight bound â€“ algorithm always scales like this (like =).
    

##### Analogy:

- O = â€œat mostâ€; Big-O means "less than or equal to (<=)"
    
- Î© = â€œat leastâ€; Big-Omega means "greater or equal to (>=)"
    
- Î˜ = â€œexactly this growthâ€; Big-Theta means "equal to (=)"
    

**Examples** :

- Insertion Sort worst case = Î˜(nÂ²).
    
- Merge Sort = Î˜(n log n).
    
- Counting Sort = Î˜(n + k).
    

âš ï¸ Exam tip: If they ask for complexity â†’ use Î˜, not just O.

![[Pasted image 20250913145553.png]]


---
# Dictionary

**[[Loop Invariant]]:**
- a condition or property about program variables that is true immediately before and after every iteration of a loop