
---
## Main Focus

- Define **algorithms** and explain why they matter.
    
- Show how algorithmic ingenuity drastically reduces computation.
    
- Introduce **divide-and-conquer** with MergeSort as the flagship example.
    
- Establish principles for **algorithm analysis**.
    

---

## 1.1 What Is an Algorithm? Why Study Them?

**Definition**: An **algorithm** is a finite, precise sequence of rules that takes an input and produces the correct output.

**Reasons to Study Algorithms**:

1. **Foundational to Computer Science**
    
    - Networking â†’ shortest paths (Dijkstra).
        
    - Cryptography â†’ modular arithmetic, number theory.
        
    - Databases â†’ hash tables, balanced search trees.
        
    - Computational biology â†’ sequence alignment via dynamic programming.
        
2. **Innovation driver**
    
    - Algorithms can create entire industries.
        
    - Example: **Googleâ€™s PageRank** algorithm revolutionized web search.
        
3. **Practicality**
    
    - Efficient algorithms make problems solvable at scale.
        
    - Inefficient ones make them infeasible.
        
4. **Career and Interviews**
    
    - Core skillset for technical interviews.
        
    - Algorithm questions test reasoning, efficiency, and problem-solving.
        

---

## 1.2 Integer Multiplication (NaÃ¯ve Method)

### Problem:

Multiply two _n_-digit integers.

### NaÃ¯ve (Grade-School) Algorithm:

- Multiply each digit of one number by each digit of the other.
    
- Running time: **O(nÂ²)** single-digit operations.
    

**Example**: Multiplying `1234 Ã— 5678` involves 16 multiplications.

ðŸ’¡ _Growth Intuition_: For n=1,000 digits â†’ ~1,000,000 operations.

---

## 1.3 Karatsubaâ€™s Algorithm (Smarter Multiplication)

### Idea: Reduce recursive multiplications.

Represent numbers split in half:

```
x = 10^(n/2)Â·a + b
y = 10^(n/2)Â·c + d
```

- where `a, b, c, d` are n/2-digit numbers.
    

NaÃ¯ve expansion:

```
xÂ·y = (10^n)(aÂ·c) + (10^(n/2))(aÂ·d + bÂ·c) + (bÂ·d)
```

Requires **4 multiplications** of size n/2.

### Karatsubaâ€™s Trick:

Instead compute:

```
(a+b)(c+d) = aÂ·c + bÂ·d + (aÂ·d + bÂ·c)
```

So:

```
aÂ·d + bÂ·c = (a+b)(c+d) â€“ aÂ·c â€“ bÂ·d
```

Now only **3 multiplications** of size n/2 are needed:

- aÂ·c
    
- bÂ·d
    
- (a+b)(c+d)
    

### Recurrence:

```
T(n) = 3T(n/2) + O(n)
```

By Master Theorem â†’ **T(n) = O(n^logâ‚‚3) â‰ˆ O(n^1.59)**.

âš¡ Speedup vs naÃ¯ve O(nÂ²).

---

## 1.4 Sorting â€“ The Problem

### Input:

Array of _n_ numbers.

### Output:

Sorted array (non-decreasing order).

### NaÃ¯ve Sorting Algorithms:

- **Selection Sort, Insertion Sort, Bubble Sort** â†’ **O(nÂ²)**.
    
- Fine for small n, but catastrophic for large n.
    

---

## 1.5 MergeSort â€“ Divide and Conquer

### Strategy:

1. **Divide** array into halves.
    
2. **Conquer** â†’ recursively sort each half.
    
3. **Combine** â†’ merge two sorted halves in O(n).
    

### Example:

Sort `[5,4,1,8,7,2,6,3]`

- Split into `[5,4,1,8]` and `[7,2,6,3]`.
    
- Sort recursively.
    
- Merge `[1,4,5,8]` + `[2,3,6,7]` â†’ `[1,2,3,4,5,6,7,8]`.
    

### Pseudocode:

```python
def merge_sort(A):
    if len(A) <= 1:
        return A
    mid = len(A)//2
    left = merge_sort(A[:mid])
    right = merge_sort(A[mid:])
    return merge(left, right)

def merge(L, R):
    result = []
    i = j = 0
    while i < len(L) and j < len(R):
        if L[i] <= R[j]:
            result.append(L[i])
            i += 1
        else:
            result.append(R[j])
            j += 1
    result.extend(L[i:])
    result.extend(R[j:])
    return result
```

---

## 1.6 MergeSort Analysis

### Recurrence:

```
T(n) = 2T(n/2) + O(n)
```

### Recursion Tree Method:

- Each level â†’ O(n) work.
    
- Number of levels â†’ logâ‚‚ n.
    
- Total = **O(n log n)**.
    

### Comparison:

- Bubble/Selection/Insertion Sort = O(nÂ²).
    
- MergeSort = O(n log n).
    

For n=1,000,000:

- Quadratic â†’ ~10^12 operations.
    
- n log n â†’ ~20Ã—10^6 (50,000Ã— faster).
    

---

## 1.7 Guiding Principles for Algorithm Analysis

1. **Worst-case analysis**
    
    - Guarantees performance for all inputs.
        
    - Easier to reason about.
        
2. **Asymptotic analysis**
    
    - Focus on input size â†’ infinity.
        
    - Ignore machine constants and lower-order terms.
        
3. **Growth rate matters most**
    
    - Linear vs quadratic is decisive.
        
    - Example: O(n) beats O(nÂ²) even if constants differ hugely.
        
4. **Divide-and-Conquer**
    
    - Solve large problems by splitting into smaller ones.
        
    - Often gives O(n log n) or better runtimes.
        

---

# ðŸ”‘ The Upshot of Chapter 1

- **Algorithms matter**: They drive technology, science, and industry.
    
- **NaÃ¯ve algorithms are not enough**: Example: O(nÂ²) integer multiplication and sorting.
    
- **Smarter approaches exist**: Karatsubaâ€™s trick (O(n^1.59)) vs O(nÂ²).
    
- **Divide-and-conquer** is powerful: MergeSort achieves O(n log n).
    
- **Analysis framework**: Use recurrences and asymptotics to reason about efficiency.
    
- **Worst-case and growth rates**: Core to algorithm design and comparison.
    

---

## ðŸŽ“ Exam / Interview Tips

- Define input/output clearly when describing an algorithm.
    
- Always compare naive vs improved solution.
    
- Be able to derive and solve recurrences.
    
- Memorize MergeSort pseudocode + O(n log n) proof.
    
- Explain Karatsubaâ€™s trick in plain words and algebra.
    
- Growth rate intuition is key: always think how runtime scales.
    

---
Next: [[Chapter 2 - Asymptotic Notation]]